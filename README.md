# 🧠 Word Embeddings in NLP: Word2Vec, GloVe, and FastText

This project provides an in-depth explanation of **Word Embeddings** in Natural Language Processing (NLP), focusing on three popular techniques: **Word2Vec**, **GloVe**, and **FastText**. These embeddings are essential tools that convert words into dense vector representations, allowing machines to understand human language more effectively.

---

## 📌 Contents

- 🔹 Introduction to Word Embeddings  
- 🔹 Word2Vec  
  - CBOW and Skip-Gram Architectures  
  - Example Use Cases  
  - Pros and Cons  
- 🔹 GloVe  
  - Co-occurrence Matrix  
  - Matrix Factorization  
  - Advantages and Limitations  
- 🔹 FastText  
  - Subword (n-gram) Representation  
  - Handling OOV Words  
  - Comparison with Word2Vec  
- 🔹 Conclusion  

---

## 📘 Summary

### 🔹 Why Word Embeddings?
Computers understand numbers, not text. Word embeddings help bridge this gap by converting words into numerical vectors that capture **semantic meaning** and **contextual relationships**.

### 🔹 Techniques Covered:
- **Word2Vec**: Predictive model that learns embeddings using local context.  
- **GloVe**: Combines local context and global co-occurrence statistics.  
- **FastText**: Builds word vectors from character-level n-grams to better handle rare and unseen words.

---

## 🛠️ Technologies Used

- Markdown (for documentation)
- Python (recommended for implementation)
- Gensim / fastText (if code is included)
- NLP concepts (CBOW, Skip-Gram, Co-occurrence, Matrix Factorization)

---

## 📈 Applications

These embedding techniques are foundational for tasks such as:
- Sentiment Analysis  
- Text Classification  
- Chatbots  
- Named Entity Recognition (NER)  
- Machine Translation

---


